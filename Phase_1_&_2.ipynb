{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Asadali016/VGG16/blob/main/Phase_1_%26_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Asadali016/VGG16.git"
      ],
      "metadata": {
        "id": "mNtp-N_YAEHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "side51sjH7og"
      },
      "outputs": [],
      "source": [
        "#Code for Extracting Features using VGG16 'Start'\n",
        "from collections import Counter\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "import skimage\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from os import listdir\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.transform import resize\n",
        "from collections import Counter\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.applications.vgg16 import VGG16 # VGG16\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Flatten, Activation, GlobalAveragePooling2D,Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XpqDaEk5E_v3"
      },
      "outputs": [],
      "source": [
        "images=[]\n",
        "labels=[]\n",
        "feature_dictionary = {\n",
        "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'label_normal': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'image': tf.io.FixedLenFeature([], tf.string)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kiIxLgXoFTPf"
      },
      "outputs": [],
      "source": [
        "def _parse_function(example, feature_dictionary=feature_dictionary):\n",
        "    parsed_example = tf.io.parse_example(example, feature_dictionary)\n",
        "    return parsed_example\n",
        "\n",
        "def read_data(filename):\n",
        "    full_dataset = tf.data.TFRecordDataset(filename,num_parallel_reads=tf.data.experimental.AUTOTUNE)\n",
        "    full_dataset = full_dataset.shuffle(buffer_size=31000)\n",
        "    full_dataset = full_dataset.cache()\n",
        "    print(\"Size of Training Dataset: \", len(list(full_dataset)))\n",
        "    \n",
        "    feature_dictionary = {\n",
        "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'label_normal': tf.io.FixedLenFeature([], tf.int64),\n",
        "    'image': tf.io.FixedLenFeature([], tf.string)\n",
        "    }   \n",
        "\n",
        "    full_dataset = full_dataset.map(_parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    print(full_dataset)\n",
        "    for image_features in full_dataset:\n",
        "        image = image_features['image'].numpy()\n",
        "        image = tf.io.decode_raw(image_features['image'], tf.uint8)\n",
        "        image = tf.reshape(image, [299, 299])        \n",
        "        image=image.numpy()\n",
        "        image=cv2.resize(image,(100,100))\n",
        "        image=cv2.merge([image,image,image])        \n",
        "        #plt.imshow(image)\n",
        "        images.append(image)\n",
        "        labels.append(image_features['label_normal'].numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_11uy7rtJVYH"
      },
      "outputs": [],
      "source": [
        "filenames=['/content/drive/MyDrive/Fiverr_WORK/training10_1/training10_1.tfrecords']\n",
        "for filenames in filenames:\n",
        "    read_data(filenames)   \n",
        "print(len(images))\n",
        "print(len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "00VI-7fnJVjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6b7b624-b4de-4462-bb0a-15f1c763b86f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11177\n",
            "11177\n"
          ]
        }
      ],
      "source": [
        "X=np.array(images)\n",
        "y=np.array(labels)\n",
        "print(len(X))\n",
        "print(len(y))\n",
        "#x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2021,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amf1xnW0J8st"
      },
      "outputs": [],
      "source": [
        "plt.imshow(X[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fMMT7bTKACE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Conv2D, MaxPool2D, Flatten\n",
        "from keras import optimizers\n",
        "from keras import losses\n",
        "from sklearn import metrics\n",
        "\n",
        "print(X[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xthmk39jKD1s"
      },
      "outputs": [],
      "source": [
        "rows, cols,color = X[0].shape\n",
        "print(X[0].shape)\n",
        "\n",
        "base_model = VGG16(input_shape=(100,100,3), weights='imagenet', include_top=False)\n",
        "x = base_model.output\n",
        "x = Dropout(0.2)(x)\n",
        "predictions = Flatten()(x)\n",
        "model_feat = Model(inputs=base_model.input,outputs=predictions)\n",
        "\n",
        "features = model_feat.predict(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "pd.DataFrame(features).to_csv('/content/drive/MyDrive/Fiverr_WORK/features.csv')"
      ],
      "metadata": {
        "id": "Lt1dfl8A-a-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd  \n",
        "features1= pd.read_csv('/content/drive/MyDrive/Fiverr_WORK/new_data.csv')\n",
        "print(len(features1))\n",
        "print(len(y))\n",
        "features1['label'] = y\n",
        "pd.DataFrame(features1).to_csv('/content/drive/MyDrive/Fiverr_WORK/new_Data.csv')\n",
        "#Code for Extracting Features using VGG16 'End'"
      ],
      "metadata": {
        "id": "DRdHRxrb73ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code for Training SVM 'Start'\n",
        "import pandas as pd\n",
        "dataset=pd.read_csv('/content/drive/MyDrive/Fiverr_WORK/New_data.csv')"
      ],
      "metadata": {
        "id": "-sSU7q9xhYgm"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=dataset.iloc[:,:-1].values\n",
        "Y=dataset.iloc[:,-1].values"
      ],
      "metadata": {
        "id": "GOk_QgUBhs_U"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.25, random_state=0)"
      ],
      "metadata": {
        "id": "wyf5IAt-h79c"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc=StandardScaler()\n",
        "X_train=sc.fit_transform(X_train)\n",
        "X_test=sc.transform(X_test)"
      ],
      "metadata": {
        "id": "d9F3LMV6iVVB"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "classifier=SVC(kernel='linear',random_state=0)\n",
        "classifier.fit(X_train,Y_train)"
      ],
      "metadata": {
        "id": "2rPIblzyiwMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred=classifier.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "cm=confusion_matrix(Y_test,Y_pred)\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "oIuZSEaYj5yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ac=accuracy_score(Y_test,Y_pred)\n",
        "print(ac)\n",
        "#Code for SVM 'End'"
      ],
      "metadata": {
        "id": "qtS9ndsPkfom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code for NADE 'Start'\n",
        "import torch\n",
        "from torch import distributions\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class NADE():\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        \"\"\"Initializes a new NADE instance.\n",
        "        Args:\n",
        "            input_dim: The dimension of the input.\n",
        "            hidden_dim: The dimmension of the hidden layer. NADE only supports one\n",
        "                hidden layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self._input_dim = input_dim\n",
        "\n",
        "        # fmt: off\n",
        "        self._in_W = nn.Parameter(torch.zeros(hidden_dim, self._input_dim))\n",
        "        self._in_b = nn.Parameter(torch.zeros(hidden_dim,))\n",
        "        self._h_W = nn.Parameter(torch.zeros(self._input_dim, hidden_dim))\n",
        "        self._h_b = nn.Parameter(torch.zeros(self._input_dim,))\n",
        "        # fmt: on\n",
        "        nn.init.kaiming_normal_(self._in_W)\n",
        "        nn.init.kaiming_normal_(self._h_W)\n",
        "\n",
        "    def _forward(self, x):\n",
        "        \"\"\"Computes the forward pass and samples a new output.\n",
        "        Returns:\n",
        "            (p_hat, x_hat) where p_hat is the probability distribution over dimensions\n",
        "            and x_hat is sampled from p_hat.\n",
        "        \"\"\"\n",
        "        # If the input is an image, flatten it during the forward pass.\n",
        "        original_shape = x.shape\n",
        "        if len(x.shape) > 2:\n",
        "            x = x.view(original_shape[0], -1)\n",
        "\n",
        "        p_hat, x_hat = [], []\n",
        "        batch_size = 1 if x is None else x.shape[0]\n",
        "        # Only the bias is used to compute the first hidden unit so we must replicate it\n",
        "        # to account for the batch size.\n",
        "        a = self._in_b.expand(batch_size, -1)\n",
        "        for i in range(self._input_dim):\n",
        "            h = torch.relu(a)\n",
        "            p_i = torch.sigmoid(h @ self._h_W[i : i + 1, :].t() + self._h_b[i : i + 1])\n",
        "            p_hat.append(p_i)\n",
        "\n",
        "            # Sample 'x' at dimension 'i' if it is not given.\n",
        "            x_i = x[:, i : i + 1]\n",
        "            x_i = torch.where(x_i < 0, distributions.Bernoulli(probs=p_i).sample(), x_i)\n",
        "            x_hat.append(x_i)\n",
        "\n",
        "            # We do not need to add self._in_b[i:i+1] when computing the other hidden\n",
        "            # units since it was already added when computing the first hidden unit.\n",
        "            a = a + x_i @ self._in_W[:, i : i + 1].t()\n",
        "        if x_hat:\n",
        "            return (\n",
        "                torch.cat(p_hat, dim=1).view(original_shape),\n",
        "                torch.cat(x_hat, dim=1).view(original_shape),\n",
        "            )\n",
        "        return []\n",
        "\n",
        "    def classifier():\n",
        "      from sklearn.naive_bayes import GaussianNB\n",
        "      c=GaussianNB()\n",
        "      return c\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Computes the forward pass.\n",
        "        Args:\n",
        "            x: Either a tensor of vectors with shape (n, input_dim) or images with shape\n",
        "                (n, 1, h, w) where h * w = input_dim.\n",
        "        Returns:\n",
        "            The result of the forward pass.\n",
        "        \"\"\"\n",
        "        return self._forward(x)[0]\n",
        "\n",
        "    def sample(self, n_samples=None, conditioned_on=None):\n",
        "        \"\"\"See the base class.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            conditioned_on = self._get_conditioned_on(n_samples, conditioned_on)\n",
        "            return self._forward(conditioned_on)[1]\n",
        "\n",
        "\n",
        "def reproduce(\n",
        "    n_epochs=50,\n",
        "    batch_size=512,\n",
        "    log_dir=\"/tmp/run\",\n",
        "    n_gpus=1,\n",
        "    device_id=0,\n",
        "    debug_loader=None,\n",
        "):\n",
        "    from torch import optim\n",
        "    from torch.nn import functional as F\n",
        "\n",
        "    from pytorch_generative import datasets\n",
        "    from pytorch_generative import models\n",
        "    from pytorch_generative import trainer\n",
        "\n",
        "    train_loader, test_loader = debug_loader, debug_loader\n",
        "    if train_loader is None:\n",
        "        train_loader, test_loader = datasets.get_mnist_loaders(\n",
        "            batch_size, dynamically_binarize=True\n",
        "        )\n",
        "\n",
        "    model = models.NADE(input_dim=784, hidden_dim=500)\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    def loss_fn(x, _, preds):\n",
        "        batch_size = x.shape[0]\n",
        "        x, preds = x.view((batch_size, -1)), preds.view((batch_size, -1))\n",
        "        loss = F.binary_cross_entropy_with_logits(preds, x, reduction=\"none\")\n",
        "        return loss.sum(dim=1).mean()\n",
        "\n",
        "    model_trainer = trainer.Trainer(\n",
        "        model=model,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        train_loader=train_loader,\n",
        "        eval_loader=test_loader,\n",
        "        log_dir=log_dir,\n",
        "        n_gpus=n_gpus,\n",
        "        device_id=device_id,\n",
        "    )\n",
        "    model_trainer.interleaved_train_and_eval(n_epochs)\n"
      ],
      "metadata": {
        "id": "zLZgngCBCwSZ"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dataset=pd.read_csv('/content/drive/MyDrive/Fiverr_WORK/positive_multi.csv')\n",
        "Data_multi=pd.read_csv('/content/drive/MyDrive/Fiverr_WORK/data.csv')"
      ],
      "metadata": {
        "id": "QPfENgT8EyIM"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_M=dataset.iloc[:,:-1].values\n",
        "Y_multi=dataset.iloc[:,-1].values"
      ],
      "metadata": {
        "id": "XbgDFD3QIbUl"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train_M,X_test_M,Y_train_M,Y_test_M=train_test_split(X_M,Y_multi,test_size=0.25, random_state=0)"
      ],
      "metadata": {
        "id": "-xTJgwvfIbUm"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc=StandardScaler()\n",
        "X_train_M=sc.fit_transform(X_train_M)\n",
        "X_test_M=sc.transform(X_test_M)"
      ],
      "metadata": {
        "id": "_x9YBbnGIbUn"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Classifier=NADE.classifier()\n",
        "Classifier.fit(X_train_M,Y_train_M)"
      ],
      "metadata": {
        "id": "THNXee7OImIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred_M=Classifier.predict(X_test_M)\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "cm=confusion_matrix(Y_test_M,Y_pred_M)\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "n18UB9c5IbUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ac=accuracy_score(Y_test_M,Y_pred_M)\n",
        "print(ac)\n",
        "#Code for NADE 'End'"
      ],
      "metadata": {
        "id": "HYQYntJYIbUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code for K-NN Classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "CLassifier=KNeighborsClassifier(n_neighbors=50,metric='minkowski',p=5) \n",
        "CLassifier.fit(X_train_M,Y_train_M)\n",
        "y_pred_1=CLassifier.predict(X_test_M)\n",
        "Cm=confusion_matrix(Y_test_M,y_pred_1)\n",
        "print(Cm)\n",
        "Ac=accuracy_score(Y_test_M,y_pred_1)\n",
        "print(Ac)\n"
      ],
      "metadata": {
        "id": "4JsLlB-V2qUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code for Testing\n",
        "actual=Y_test[10]\n",
        "actual_M=Y_test_M[10]\n",
        "print('Original Values',actual)\n",
        "print('Original Multi_class Value',actual_M)\n",
        "input=X_test[10,:]\n",
        "input_M=X_test_M[10,:]\n",
        "input=input.reshape(1,-1)\n",
        "input_M=input_M.reshape(1,-1)\n",
        "check=classifier.predict(input)\n",
        "if(check==0):\n",
        "  print('Output=Negative')\n",
        "else :\n",
        "  print('Output=Positive')\n",
        "  check1=Classifier.predict(input_M)\n",
        "  print('Predicted by NADE',check1)\n",
        "  check2=CLassifier.predict(input_M)\n",
        "  print('Predicted by K-NN',check2)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqHq6EVl59yU",
        "outputId": "83f4b9ee-9f03-4838-8c94-687afa1d7a49"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Values 0\n",
            "Original Multi_class Value 3\n",
            "Output=Negative\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "11qJQJxmEyGaQHz-SJo9c9izYKjCOuae5",
      "authorship_tag": "ABX9TyMwD5SqsfqP0iGwh/NPHx/J",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}